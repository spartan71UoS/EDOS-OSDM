{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aad6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import cuda\n",
    "import torch\n",
    "from transformers import DistilBertConfig\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('edos_labelled_aggregated.csv')\n",
    "labels = data['label_sexist'].values\n",
    "categories = data['label_category'].values\n",
    "vectors = data['label_vector'].values\n",
    "texts = data['text'].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels, train_categories, test_categories, train_vectors, test_vectors = train_test_split(\n",
    "    texts, labels, categories, vectors, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the label mapping\n",
    "label_mapping = {\n",
    "    'sexist': 1,\n",
    "    'not sexist': 0\n",
    "}\n",
    "\n",
    "category_mapping = {\n",
    "    'none': 0,\n",
    "    '1. threats, plans to harm and incitement': 1,\n",
    "    '2. derogation': 2,\n",
    "    '3. animosity': 3,\n",
    "    '4. prejudiced discussions': 4\n",
    "\n",
    "    # Add more categories as necessary\n",
    "}\n",
    "\n",
    "vector_mapping = {\n",
    "    'none': 0,\n",
    "    '1.1 threats of harm': 1,\n",
    "    '1.2 incitement and encouragement of harm': 2,\n",
    "    '2.1 descriptive attacks': 3,\n",
    "    '2.2 aggressive and emotive attacks': 4,\n",
    "    '2.3 dehumanising attacks & overt sexual objectification': 5,\n",
    "    '3.1 casual use of gendered slurs, profanities, and insults': 6,\n",
    "    '3.2 immutable gender differences and gender stereotypes': 7,\n",
    "    '3.3 backhanded gendered compliments': 8,\n",
    "    '3.4 condescending explanations or unwelcome advice': 9,\n",
    "    '4.1 supporting mistreatment of individual women': 10,\n",
    "    '4.2 supporting systemic discrimination against women as a group': 11\n",
    "    \n",
    "\n",
    "    # Add more categories as necessary\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34737027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, categories, vectors, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.categories = categories\n",
    "        self.vectors = vectors\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        category = self.categories[idx]\n",
    "        vector = self.vectors[idx]\n",
    "    \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label_mapping[label]),  # Encode label as numerical value\n",
    "            'category': torch.tensor(category_mapping[category]),  # Encode category as numerical value\n",
    "            'vector': torch.tensor(vector_mapping[vector])\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2a00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set hyperparameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDistilBertForSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size + 1 + 1 + 1 , config.num_labels)  # Include 1 additional unit for each extra feature\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, category=None, labels=None, vector=None, **kwargs):\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        hidden_state = distilbert_output.last_hidden_state[:, 0, :]  # Extract the [CLS] token embedding\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "    \n",
    "        # Reshape the category tensor to match the dimensions of the hidden_state tensor\n",
    "        if category is not None:\n",
    "            category = category.unsqueeze(1)  # Add an extra dimension\n",
    "        \n",
    "        # Reshape the labels tensor to match the dimensions of the hidden_state tensor\n",
    "        if labels is not None:\n",
    "            labels = labels.unsqueeze(1)  # Add an extra dimension\n",
    "    \n",
    "        # Reshape the vector tensor to match the dimensions of the hidden_state tensor\n",
    "        if vector is not None:\n",
    "            vector = vector.unsqueeze(1)  # Add an extra dimension\n",
    "    \n",
    "        # Concatenate the hidden state with the extra features\n",
    "        if category is not None:\n",
    "            hidden_state = torch.cat((hidden_state, category), dim=1)\n",
    "        if labels is not None:\n",
    "            hidden_state = torch.cat((hidden_state, labels), dim=1)\n",
    "        if vector is not None:\n",
    "            hidden_state = torch.cat((hidden_state, vector), dim=1)\n",
    "    \n",
    "        logits = self.classifier(hidden_state)\n",
    "        outputs = (logits,) + distilbert_output[1:]  # Add hidden states and attention if they are present\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc08f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average Loss: 0.4972\n",
      "Test Loss: 0.40456696376204493, Accuracy: 0.82725, Precision: 0.7796610169491526, Recall: 0.38534031413612563, F1: 0.5157673440784862\n",
      "Epoch 2/5, Average Loss: 0.3447\n",
      "Test Loss: 0.36600663208961487, Accuracy: 0.841, Precision: 0.6952264381884945, Recall: 0.5947643979057592, F1: 0.6410835214446953\n",
      "Epoch 3/5, Average Loss: 0.2623\n",
      "Test Loss: 0.35711890085041526, Accuracy: 0.84675, Precision: 0.7544642857142857, Recall: 0.5308900523560209, F1: 0.6232329440688383\n",
      "Epoch 4/5, Average Loss: 0.1996\n",
      "Test Loss: 0.39021957623958586, Accuracy: 0.85, Precision: 0.7278562259306803, Recall: 0.5937172774869109, F1: 0.6539792387543253\n",
      "Epoch 5/5, Average Loss: 0.1511\n",
      "Test Loss: 0.3791333614215255, Accuracy: 0.84125, Precision: 0.6886792452830188, Recall: 0.6115183246073298, F1: 0.6478092068774266\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = CustomDistilBertForSequenceClassification(config)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TextDataset(train_texts, train_labels, train_categories, train_vectors, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, test_categories, test_vectors, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Set optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        category = batch['category'].to(device)\n",
    "        vector = batch['vector'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            category=category,\n",
    "            vector=vector\n",
    "        )\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        loss = criterion(logits, labels)  # Calculate the loss\n",
    "        \n",
    "               \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            category = batch['category'].to(device)\n",
    "            vector = batch['vector'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                category=category,\n",
    "                vector=vector\n",
    "            )\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            optimizer.step()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            softmax_probs = torch.softmax(logits, dim=1)\n",
    "            predicted_labels = torch.argmax(softmax_probs, dim=1)\n",
    "            \n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            predictions.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    print(f'Test Loss: {avg_test_loss}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdcf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce1c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
